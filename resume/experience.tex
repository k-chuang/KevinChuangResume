%-------------------------------------------------------------------------------
%	SECTION TITLE
%-------------------------------------------------------------------------------
\cvsection{Work Experience}


%-------------------------------------------------------------------------------
%	CONTENT
%-------------------------------------------------------------------------------
\begin{cventries}

% %---------------------------------------------------------
\cventryupdate
    % {Applied Machine Learning Group, Machine Learning Engineer} % Job title
    {Senior Data Engineer} % second title
    {HomeLight} % Organization
    {San Francisco, CA} % Location
    {07/2022 - 04/2023} % second date
    {Data Engineer} % second title
    {11/2020 - 07/2022} % Date(s)
    {
      \begin{cvitems} % Description(s) of tasks/responsibilities
        % \item{Designed, researched, and implemented Bayesian Logistic Regression using CTR data from Google BigQuery to help boost carousel scores for movies & TV show recommendations}
        % \item{Implemented ETL pipeline with multiple data sources for ad tracking to enable marketing team and reduce agency ad costs using \textbf{Django}, \textbf{Airflow} \& \textbf{SQL} on \textbf{AWS} (\textbf{ECS}, \textbf{S3}, \textbf{RedShift})}
        % \item{Designed and implemented in-house ad tracking platform to enable marketing team and reduce agency ad costs using \textbf{Django}, \textbf{Airflow} \& \textbf{SQL} on \textbf{AWS} (\textbf{ECS}, \textbf{S3}, \textbf{RedShift})}
        % \item{Designed, built, and launched core algorithm to rank and match real estate agents with home buyer and seller leads using \textbf{Python}, \textbf{SQL}, \textbf{Django}, \textbf{ElasticSearch}, \textbf{Airflow}, \textbf{AWS (Kinesis, S3, Redshift)} and \textbf{Heroku}, \textbf{boosting lead conversion rate by 15\%}}
        % \item{Developed internal TV ad planning and bid optimization system  using \textbf{Python, Django, Airflow, SQL, AWS (ECS, S3, RedShift)}, and \textbf{Heroku} to support marketing team, \textbf{resulting in reduction of \$250,000 annually in external agency ad costs}}
        \item{Designed, built, and launched core algorithm to match and rank real estate agents with home buyer and seller opportunities, \textbf{boosting conversion rate by 15\%} using \textbf{Python}, \textbf{SQL}, \textbf{Django}, \textbf{ElasticSearch}, \textbf{Airflow}, \textbf{AWS (Kinesis, S3, Redshift)} and \textbf{Heroku}}
        \item{Developed internal TV ad planning and bid optimization system to support marketing team, \textbf{resulting in reduction of \$250,000 annually in external ad agency costs} using \textbf{Python, Django, Airflow, SQL, AWS (ECS, S3, RedShift)}, and \textbf{Heroku}}
         \item{Spearheaded design and deployment of mission-critical data pipelines to showcase property listing images and transaction history, \textbf{enhancing user experience on company's primary website} using \textbf{Python, Airflow, SQL, ElasticSearch, Kafka} and \textbf{AWS (Lambda, SQS, S3, Redshift, Kinesis, DynamoDB)}}
         \item{Implemented record linkage solution to support operations team in tracking real estate agents and transactions, \textbf{leading to discovery of missed revenue opportunities totaling over \$500,000} utilizing \textbf{Python, SQL, Airflow, Redshift}, and \textbf{ElasticSearch}}
         % \item{Implemented internal TV ad planning and bid optimization system to support marketing team and \textbf{significantly reduce agency ad costs by \$250,000} per year using \textbf{Python}, \textbf{Django}, \textbf{Airflow}, \textbf{SQL}, \textbf{AWS} (\textbf{ECS}, \textbf{S3}, \textbf{RedShift}) and \textbf{Heroku}}
          % \item{Developed record linkage solution of disparate data sources to support operations team in keeping track of real estate agents \& transactions and uncovered \textbf{hundreds of thousands of dollars in missed potential revenue} using \textbf{Python, SQL, Airflow, Redshift, ElasticSearch}}
        % \item{Designed, implemented, and deployed data processing pipelines to process \& expose US property data (property images, property history) supporting production HomeLight website pages using \textbf{Python}, \textbf{Airflow}, \textbf{SQL}, \textbf{ElasticSearch} and \textbf{AWS (Lambda, SQS, S3, Redshift, DynamoDB)}}  
        % \item{Architected and standardized reusable pattern for moving data across different data sources for other cross-functional teams to use using Python, Airflow, Logstash, Elasticsearch, and AWS (ECS, Redshift)}
        % \item{Designed, built, and launched core data pipelines to ingest and process critical MLS data across the US to support property listing pages (property search, property history) using \textbf{Python}, \textbf{Airflow}, \textbf{SQL}, \textbf{Kafka} and \textbf{ElasticSearch}}
        % \item{Designed \& developed media data pipeline to process and expose property images on HomeLight website using \textbf{Python}, \textbf{Airflow} and \textbf{AWS (Lambda, SQS, S3, Redshift, DynamoDB, CloudWatch)}}
        % \item{Developed record linkage solution of disparate data sources to support operations team in keeping track of real estate agents and transactions to discover hundreds of thousands of dollars in missed revenue using \textbf{Python, Airflow, Redshift, ElasticSearch}}
        % \item{Designed \& developed batch new media system to improve processing of large batches of property images using \textbf{Airflow} and \textbf{AWS (Lambda, SQS, S3, Redshift, DynamoDB, CloudWatch)}}
        % \item{Contributed to internal data platform framework by creating reusable components for data ingestion \& validation}
        % \item{Contributed to ETL pipeline to power real estate properties across US}
%  		\item {Designed \& implemented ML service to automatically generate intelligent video previews using \textbf{Django}, \textbf{Docker}, and \textbf{Kubernetes} / \textbf{Helm} and productionized, maintained, and monitored service on \textbf{GCP} (\textbf{Stackdriver}, \textbf{GKE}, \textbf{Pub/Sub}, \textbf{GCS}, \textbf{Cloud SQL}), \textbf{Codefresh}, \textbf{Grafana} and \textbf{Prometheus} to save company money from using external services and support internal brands}
%  		\item {Designed \& implemented ML service to automatically generate intelligent video previews using \textbf{Django}, \textbf{Docker}, and \textbf{Kubernetes} / \textbf{Helm} and productionized \& maintained service on \textbf{Google Cloud Platform}, \textbf{Codefresh}, \textbf{Grafana} and \textbf{Prometheus} to save company money from using external services and support internal brands}
%   		\item {Built serverless ALS collaborative filtering recommendation system using \textbf{Spark}, \textbf{Docker}, \textbf{Kubernetes}, and \textbf{Google Cloud Platform}}
%  		\item {Built serverless ALS collaborative filtering recommendation system using \textbf{Spark}, \textbf{Docker}, \textbf{Kubernetes}, and \textbf{GCP} (\textbf{BigQuery}, \textbf{Dataproc}, \textbf{GKE})}
  		% \item {Built serverless ALS collaborative filtering recommendation system using \textbf{PySpark}, \textbf{Docker}, \textbf{Kubernetes}, and \textbf{GCP} (\textbf{BigQuery}, \textbf{Dataproc}, \textbf{GKE}) and utilized \textbf{Argo} workflows to automate services in the pipeline}
    %   \item {Contributed to }
        % \item {Designed \& implemented API layer of automatic video preview \& thumbnail generator microservice using \textbf{Django}, \textbf{Docker}, and \textbf{Kubernetes}}
        % \item {Productionized, maintained, and monitored automatic video preview \& thumbnail generator service on \textbf{GCP} (\textbf{Stackdriver}, \textbf{GKE}, \textbf{Pub/Sub}, \textbf{GCS}, \textbf{Cloud SQL}), \textbf{Codefresh}, and \textbf{Grafana} to save company money and support our brands (e.g. \textbf{CBS News}, \textbf{CBS All Access}) to use service}
      \end{cvitems}
    }

% % %---------------------------------------------------------
\cventryupdate
    % {Applied Machine Learning Group, Machine Learning Engineer} % Job title
    {Machine Learning Engineer} % Job title
    % {CBS Interactive} % Organization
    {ViacomCBS} % Organization
    {San Francisco, CA} % Location
    {02/2020 - 11/2020} % Date(s)
    {Data Science Intern} % second title
    {06/2019 - 12/2019} % second date
    {
      \begin{cvitems} % Description(s) of tasks/responsibilities
        % \item{Designed, researched, and implemented Bayesian Logistic Regression using CTR data from Google BigQuery to help boost carousel scores for movies & TV show recommendations}
%  		\item {Designed \& implemented ML service to automatically generate intelligent video previews using \textbf{Django}, \textbf{Docker}, and \textbf{Kubernetes} / \textbf{Helm} and productionized, maintained, and monitored service on \textbf{GCP} (\textbf{Stackdriver}, \textbf{GKE}, \textbf{Pub/Sub}, \textbf{GCS}, \textbf{Cloud SQL}), \textbf{Codefresh}, \textbf{Grafana}, and \textbf{Prometheus} to save company money from using external services and support internal brands}
        % \item {Designed, built, and launched machine learning application to automatically generate and expose engaging video previews using \textbf{Python}, \textbf{Django}, \textbf{Docker}, \textbf{Kubernetes} / \textbf{Helm}, \textbf{GCP} (\textbf{Stackdriver}, \textbf{GKE}, \textbf{Pub/Sub}, \textbf{GCS}, \textbf{Cloud SQL}), \textbf{Codefresh} and \textbf{Grafana} to support internal brands (\textbf{CBS News}, \textbf{CBS All Access}) and \textbf{remove dependency on external services, resulting in significant cost savings}}
            \item {Designed, built, and launched machine learning application to automatically generate and expose engaging video previews to support internal brands (\textbf{CBS News}, \textbf{CBS All Access}) and remove dependency on external services, \textbf{resulting in significant cost savings} using \textbf{Python}, \textbf{Django}, \textbf{Kubernetes} / \textbf{Helm}, and \textbf{GCP} (\textbf{GKE}, \textbf{Pub/Sub}, \textbf{GCS}, \textbf{Cloud SQL})}
 		\item {Built serverless collaborative filtering recommendation system to power personalized movie / show recommendations for \textbf{CBS All Access} using \textbf{Python}, \textbf{Spark}, \textbf{Redis}, \textbf{Kubernetes}, \textbf{Argo} and \textbf{GCP} (\textbf{BigQuery}, \textbf{Dataproc}, \textbf{GKE})}
        \item {Developed and launched backend API for creating personalized movie / show recommendation carousels for millions of new \& trial users on \textbf{CBS All Access}, \textbf{enhancing user experience} using \textbf{Python}, \textbf{Django}, \textbf{Redis}, \textbf{Kubernetes}, and \textbf{GCP} (\textbf{GKE}, \textbf{Cloud SQL})}
  		\item {Researched various text summarization methods (TextRank, Pointer Generator) to create cloud-native text summarizer application consisting of Chrome extension (\textbf{JavaScript}) with \textbf{Python Flask} backend server deployed on \textbf{Google App Engine} using \textbf{Docker}}
  		% \item {Built serverless ALS collaborative filtering recommendation system using \textbf{PySpark}, \textbf{Docker}, \textbf{Kubernetes}, and \textbf{GCP} (\textbf{BigQuery}, \textbf{Dataproc}, \textbf{GKE}) and utilized \textbf{Argo} workflows to automate services in the pipeline}
        % \item {Designed \& implemented API layer of automatic video preview \& thumbnail generator microservice using \textbf{Django}, \textbf{Docker}, and \textbf{Kubernetes}}
        % \item {Productionized, maintained, and monitored automatic video preview \& thumbnail generator service on \textbf{GCP} (\textbf{Stackdriver}, \textbf{GKE}, \textbf{Pub/Sub}, \textbf{GCS}, \textbf{Cloud SQL}), \textbf{Codefresh}, and \textbf{Grafana} to save company money and support our brands (e.g. \textbf{CBS News}, \textbf{CBS All Access}) to use service}
%  		\item {Researched various text summarization methods (TextRank, Pointer Generator) to create cloud-native text summarizer application consisting of Chrome extension (\textbf{JavaScript}) with \textbf{Python Flask} backend server deployed on \textbf{Google App Engine} using \textbf{Docker}}
  		% \item {Architected \& built a cloud-native ML microservice for automatic CNET host recognition from videos using \textbf{GCP} (\textbf{AutoML, Pub/Sub, CloudSQL, GCS}), \textbf{Docker}, and \textbf{Django}}
%   		\item {Researched, implemented, and benchmarked different text summarization methods (TextRank, Pointer Generator)}
% 		\item {Created a Google Chrome extension (JavaScript/HTML/CSS) with a Python backend server to demonstrate the different text summarization methods}
      \end{cvitems}
    }


% % %---------------------------------------------------------
% \cventry
%     % {Applied Machine Learning Group, Machine Learning Engineer} % Job title
%     {Machine Learning Engineer, Applied Machine Learning Group} % Job title
%     {ViacomCBS} % Organization
%     {San Francisco, CA} % Location
%     {Feb. 2020 - Present} % Date(s)
%     {
%       \begin{cvitems} % Description(s) of tasks/responsibilities
%         % \item{Designed, researched, and implemented Bayesian Logistic Regression using CTR data from Google BigQuery to help boost carousel scores for movies & TV show recommendations}
%  		\item {Designed \& implemented ML service to automatically generate intelligent video previews using \textbf{Django}, \textbf{Docker}, and \textbf{Kubernetes} / \textbf{Helm} and productionized, maintained, and monitored service on \textbf{GCP} (\textbf{Stackdriver}, \textbf{GKE}, \textbf{Pub/Sub}, \textbf{GCS}, \textbf{Cloud SQL}), \textbf{Codefresh}, \textbf{Grafana} and \textbf{Prometheus} to save company money from using external services and support internal brands}
% %  		\item {Designed \& implemented ML service to automatically generate intelligent video previews using \textbf{Django}, \textbf{Docker}, and \textbf{Kubernetes} / \textbf{Helm} and productionized \& maintained service on \textbf{Google Cloud Platform}, \textbf{Codefresh}, \textbf{Grafana} and \textbf{Prometheus} to save company money from using external services and support internal brands}
% %   		\item {Built serverless ALS collaborative filtering recommendation system using \textbf{Spark}, \textbf{Docker}, \textbf{Kubernetes}, and \textbf{Google Cloud Platform}}
% %  		\item {Built serverless ALS collaborative filtering recommendation system using \textbf{Spark}, \textbf{Docker}, \textbf{Kubernetes}, and \textbf{GCP} (\textbf{BigQuery}, \textbf{Dataproc}, \textbf{GKE})}
%   		\item {Built serverless ALS collaborative filtering recommendation system using \textbf{PySpark}, \textbf{Docker}, \textbf{Kubernetes}, and \textbf{GCP} (\textbf{BigQuery}, \textbf{Dataproc}, \textbf{GKE}) and utilized \textbf{Argo} workflows to automate services in the pipeline}
%     %   \item {Contributed to }
%         % \item {Designed \& implemented API layer of automatic video preview \& thumbnail generator microservice using \textbf{Django}, \textbf{Docker}, and \textbf{Kubernetes}}
%         % \item {Productionized, maintained, and monitored automatic video preview \& thumbnail generator service on \textbf{GCP} (\textbf{Stackdriver}, \textbf{GKE}, \textbf{Pub/Sub}, \textbf{GCS}, \textbf{Cloud SQL}), \textbf{Codefresh}, and \textbf{Grafana} to save company money and support our brands (e.g. \textbf{CBS News}, \textbf{CBS All Access}) to use service}
%       \end{cvitems}
%     }




% %---------------------------------------------------------
% \cventry
%     % {Applied Machine Learning Group, Machine Learning Engineer} % Job title
%     {Data Science Intern, Applied Machine Learning Group} % second title
%     {CBS Interactive} % Organization
%     {San Francisco, CA} % Location
%     {June 2019 - Dec. 2019} % second date
%     {
%       \begin{cvitems} % Description(s) of tasks/responsibilities
%  		\item {Researched various text summarization methods (TextRank, Pointer Generator) and created production-ready text summarizer application consisting of Chrome extension (\textbf{JavaScript/HTML/CSS}) with Python backend server deployed on \textbf{Google App Engine} using \textbf{Docker}}
%   		\item {Built ML microservice for automatic person recognition from videos using \textbf{GCP} (\textbf{AutoML, Pub/Sub, CloudSQL, GCS}), \textbf{Docker}, and \textbf{Django}}
% %   		\item {Researched, implemented, and benchmarked different text summarization methods (TextRank, Pointer Generator)}
% % 		\item {Created a Google Chrome extension (JavaScript/HTML/CSS) with a Python backend server to demonstrate the different text summarization methods}
%       \end{cvitems}
%     }

%---------------------------------------------------------
% \cventry
%     % {Audio Test \& Data Team, Data Engineer} % Job title
%     {Data Engineer, Audio Test \& Data Team} % Job title
%     {Knowles Intelligent Audio} % Organization
%     {Mountain View, CA} % Location
%     {Oct. 2016 - May 2019} % Date(s)
%     {
%       \begin{cvitems} % Description(s) of tasks/responsibilities
% %         \item {Developed Matlab and Python tools to automate data collection}
% %         \item {Developed Python tools \& scripts to validate, process, clean, and upload large amounts of critical customer and internal audio data to MongoDB database to enable other cross-functional engineering teams to use data for testing and model training}
% % 	\item {Designed data pipeline using Python for verification, processing, and transfer of large amounts of critical audio data into \textbf{MongoDB} database}
%  		\item {Designed \& implemented data pipeline using \textbf{Python} for verification, processing, and transfer of large amounts of critical audio data into \textbf{MongoDB} database to enable cross-functional engineering teams and created \textbf{REST API} for CRUD operations on the database}
%  	       \item {Developed programmatic tools for audio data acquisition, data analysis and data processing using \textbf{Python} and \textbf{MATLAB}}
%         % \item {Developed tools to automate data acquisition \& data processing and created clear documentation \& detailed procedures for usage of these tools}
%         % \item {Contributed to \textbf{Python REST API} for CRUD operations and designed functional test suites for \textbf{MongoDB} database}
%         % \item {Tested various audio devices for audio quality and reported}
%         % \item {Designed and implemented automated functional and usability test suites for web-based database application using Selenium and Python (unittest, pytest)}
%       \end{cvitems}
%     }



% %---------------------------------------------------------
\cventryupdate
    % {Audio Test \& Data Team, Data Engineer} % Job title
    {Data Engineer} % Job title
    {Knowles Intelligent Audio} % Organization
    {Mountain View, CA} % Location
    {02/2017 - 05/2019} % Date(s)
    {Test Engineering Intern} % second title
    {10/2016 - 02/2017} % second date
    {
      \begin{cvitems} % Description(s) of tasks/responsibilities
%         \item {Developed Matlab and Python tools to automate data collection}
%         \item {Developed Python tools \& scripts to validate, process, clean, and upload large amounts of critical customer and internal audio data to MongoDB database to enable other cross-functional engineering teams to use data for testing and model training}
% 	\item {Designed data pipeline using Python for verification, processing, and transfer of large amounts of critical audio data into \textbf{MongoDB} database}
 		\item {Designed \& implemented \textbf{Python} data pipelines for verification, processing, and transfer of large amounts of critical audio data into \textbf{MongoDB} database to enable cross-functional engineering teams}
 	      % \item {Developed programmatic tools for audio data acquisition, data analysis and data processing using \textbf{Python} \& \textbf{MATLAB}}
        % \item {Developed tools to automate data acquisition \& data processing and created clear documentation \& detailed procedures for usage of these tools}
        % \item {Contributed to \textbf{Python REST API} for CRUD operations and designed functional test suites for \textbf{MongoDB} database}
        % \item {Proficiently used}
        % \item {Designed and implemented automated functional and usability test suites for web-based database application using Selenium and Python (unittest, pytest)}
      \end{cvitems}
    }

%---------------------------------------------------------
\end{cventries}
