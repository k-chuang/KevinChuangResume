%-------------------------------------------------------------------------------
%	SECTION TITLE
%-------------------------------------------------------------------------------
\cvsection{Work Experience}


%-------------------------------------------------------------------------------
%	CONTENT
%-------------------------------------------------------------------------------
\begin{cventries}

% %---------------------------------------------------------
\cventry
    % {Applied Machine Learning Group, Machine Learning Engineer} % Job title
    {Data Engineer} % Job title
    {HomeLight} % Organization
    {San Francisco, CA} % Location
    {Nov. 2020 - Present} % Date(s)
    {
      \begin{cvitems} % Description(s) of tasks/responsibilities
        % \item{Designed, researched, and implemented Bayesian Logistic Regression using CTR data from Google BigQuery to help boost carousel scores for movies & TV show recommendations}
        % \item{Implemented ETL pipeline with multiple data sources for ad tracking to enable marketing team and reduce agency ad costs using \textbf{Django}, \textbf{Airflow} \& \textbf{SQL} on \textbf{AWS} (\textbf{ECS}, \textbf{S3}, \textbf{RedShift})}
        % \item{Designed and implemented in-house ad tracking platform to enable marketing team and reduce agency ad costs using \textbf{Django}, \textbf{Airflow} \& \textbf{SQL} on \textbf{AWS} (\textbf{ECS}, \textbf{S3}, \textbf{RedShift})}
        \item{Built \& productionized internal TV ad tracking \& bid optimization system with multiple data sources to enable marketing team and significantly reduce agency ad costs using \textbf{Django}, \textbf{Airflow} \& \textbf{SQL} on \textbf{AWS} (\textbf{ECS}, \textbf{S3}, \textbf{RedShift}) and \textbf{Heroku}}
        \item{Contributed to design, implementation, and productionization of core algorithm to rank real estate agents and match them with home buyers \& sellers using \textbf{Django}, \textbf{ElasticSearch},  \& \textbf{SQL} on \textbf{AWS (Kinesis, S3, Redshift)} and \textbf{Heroku}}
        % \item{Contributed to internal data platform framework by creating reusable components for data ingestion \& validation}
        % \item{Contributed to ETL pipeline to power real estate properties across US}
%  		\item {Designed \& implemented ML service to automatically generate intelligent video previews using \textbf{Django}, \textbf{Docker}, and \textbf{Kubernetes} / \textbf{Helm} and productionized, maintained, and monitored service on \textbf{GCP} (\textbf{Stackdriver}, \textbf{GKE}, \textbf{Pub/Sub}, \textbf{GCS}, \textbf{Cloud SQL}), \textbf{Codefresh}, \textbf{Grafana} and \textbf{Prometheus} to save company money from using external services and support internal brands}
%  		\item {Designed \& implemented ML service to automatically generate intelligent video previews using \textbf{Django}, \textbf{Docker}, and \textbf{Kubernetes} / \textbf{Helm} and productionized \& maintained service on \textbf{Google Cloud Platform}, \textbf{Codefresh}, \textbf{Grafana} and \textbf{Prometheus} to save company money from using external services and support internal brands}
%   		\item {Built serverless ALS collaborative filtering recommendation system using \textbf{Spark}, \textbf{Docker}, \textbf{Kubernetes}, and \textbf{Google Cloud Platform}}
%  		\item {Built serverless ALS collaborative filtering recommendation system using \textbf{Spark}, \textbf{Docker}, \textbf{Kubernetes}, and \textbf{GCP} (\textbf{BigQuery}, \textbf{Dataproc}, \textbf{GKE})}
  		% \item {Built serverless ALS collaborative filtering recommendation system using \textbf{PySpark}, \textbf{Docker}, \textbf{Kubernetes}, and \textbf{GCP} (\textbf{BigQuery}, \textbf{Dataproc}, \textbf{GKE}) and utilized \textbf{Argo} workflows to automate services in the pipeline}
    %   \item {Contributed to }
        % \item {Designed \& implemented API layer of automatic video preview \& thumbnail generator microservice using \textbf{Django}, \textbf{Docker}, and \textbf{Kubernetes}}
        % \item {Productionized, maintained, and monitored automatic video preview \& thumbnail generator service on \textbf{GCP} (\textbf{Stackdriver}, \textbf{GKE}, \textbf{Pub/Sub}, \textbf{GCS}, \textbf{Cloud SQL}), \textbf{Codefresh}, and \textbf{Grafana} to save company money and support our brands (e.g. \textbf{CBS News}, \textbf{CBS All Access}) to use service}
      \end{cvitems}
    }

% % %---------------------------------------------------------
\cventryupdate
    % {Applied Machine Learning Group, Machine Learning Engineer} % Job title
    {Machine Learning Engineer} % Job title
    % {CBS Interactive} % Organization
    {ViacomCBS} % Organization
    {San Francisco, CA} % Location
    {Feb. 2020 - Nov. 2020} % Date(s)
    {Data Science Intern} % second title
    {June 2019 - Dec. 2019} % second date
    {
      \begin{cvitems} % Description(s) of tasks/responsibilities
        % \item{Designed, researched, and implemented Bayesian Logistic Regression using CTR data from Google BigQuery to help boost carousel scores for movies & TV show recommendations}
%  		\item {Designed \& implemented ML service to automatically generate intelligent video previews using \textbf{Django}, \textbf{Docker}, and \textbf{Kubernetes} / \textbf{Helm} and productionized, maintained, and monitored service on \textbf{GCP} (\textbf{Stackdriver}, \textbf{GKE}, \textbf{Pub/Sub}, \textbf{GCS}, \textbf{Cloud SQL}), \textbf{Codefresh}, \textbf{Grafana}, and \textbf{Prometheus} to save company money from using external services and support internal brands}
        \item {Designed, implemented, and productionized ML service to automatically generate intelligent video previews using \textbf{Django}, \textbf{Docker}, and \textbf{Kubernetes} / \textbf{Helm} on \textbf{GCP} (\textbf{Stackdriver}, \textbf{GKE}, \textbf{Pub/Sub}, \textbf{GCS}, \textbf{Cloud SQL}), \textbf{Codefresh} and \textbf{Grafana} to save company money from using external services and support internal brands (\textbf{CBS News}, \textbf{CBS All Access})}
 		\item {Built serverless ALS collaborative filtering recommendation system using \textbf{Spark}, \textbf{Redis}, \textbf{Kubernetes}, and \textbf{GCP} (\textbf{BigQuery}, \textbf{Dataproc}, \textbf{GKE}) to power personalized movie / show recommendations for \textbf{CBS All Access}}
  		% \item {Built serverless ALS collaborative filtering recommendation system using \textbf{PySpark}, \textbf{Docker}, \textbf{Kubernetes}, and \textbf{GCP} (\textbf{BigQuery}, \textbf{Dataproc}, \textbf{GKE}) and utilized \textbf{Argo} workflows to automate services in the pipeline}
        % \item {Designed \& implemented API layer of automatic video preview \& thumbnail generator microservice using \textbf{Django}, \textbf{Docker}, and \textbf{Kubernetes}}
        % \item {Productionized, maintained, and monitored automatic video preview \& thumbnail generator service on \textbf{GCP} (\textbf{Stackdriver}, \textbf{GKE}, \textbf{Pub/Sub}, \textbf{GCS}, \textbf{Cloud SQL}), \textbf{Codefresh}, and \textbf{Grafana} to save company money and support our brands (e.g. \textbf{CBS News}, \textbf{CBS All Access}) to use service}
%  		\item {Researched various text summarization methods (\textbf{TextRank, Pointer Generator}) and created a cloud-native text summarizer application consisting of Chrome extension (\textbf{JavaScript/HTML/CSS}) with Python backend server deployed on \textbf{Google App Engine} using \textbf{Docker}}
  		% \item {Architected \& built a cloud-native ML microservice for automatic CNET host recognition from videos using \textbf{GCP} (\textbf{AutoML, Pub/Sub, CloudSQL, GCS}), \textbf{Docker}, and \textbf{Django}}
%   		\item {Researched, implemented, and benchmarked different text summarization methods (TextRank, Pointer Generator)}
% 		\item {Created a Google Chrome extension (JavaScript/HTML/CSS) with a Python backend server to demonstrate the different text summarization methods}
      \end{cvitems}
    }


% % %---------------------------------------------------------
% \cventry
%     % {Applied Machine Learning Group, Machine Learning Engineer} % Job title
%     {Machine Learning Engineer, Applied Machine Learning Group} % Job title
%     {ViacomCBS} % Organization
%     {San Francisco, CA} % Location
%     {Feb. 2020 - Present} % Date(s)
%     {
%       \begin{cvitems} % Description(s) of tasks/responsibilities
%         % \item{Designed, researched, and implemented Bayesian Logistic Regression using CTR data from Google BigQuery to help boost carousel scores for movies & TV show recommendations}
%  		\item {Designed \& implemented ML service to automatically generate intelligent video previews using \textbf{Django}, \textbf{Docker}, and \textbf{Kubernetes} / \textbf{Helm} and productionized, maintained, and monitored service on \textbf{GCP} (\textbf{Stackdriver}, \textbf{GKE}, \textbf{Pub/Sub}, \textbf{GCS}, \textbf{Cloud SQL}), \textbf{Codefresh}, \textbf{Grafana} and \textbf{Prometheus} to save company money from using external services and support internal brands}
% %  		\item {Designed \& implemented ML service to automatically generate intelligent video previews using \textbf{Django}, \textbf{Docker}, and \textbf{Kubernetes} / \textbf{Helm} and productionized \& maintained service on \textbf{Google Cloud Platform}, \textbf{Codefresh}, \textbf{Grafana} and \textbf{Prometheus} to save company money from using external services and support internal brands}
% %   		\item {Built serverless ALS collaborative filtering recommendation system using \textbf{Spark}, \textbf{Docker}, \textbf{Kubernetes}, and \textbf{Google Cloud Platform}}
% %  		\item {Built serverless ALS collaborative filtering recommendation system using \textbf{Spark}, \textbf{Docker}, \textbf{Kubernetes}, and \textbf{GCP} (\textbf{BigQuery}, \textbf{Dataproc}, \textbf{GKE})}
%   		\item {Built serverless ALS collaborative filtering recommendation system using \textbf{PySpark}, \textbf{Docker}, \textbf{Kubernetes}, and \textbf{GCP} (\textbf{BigQuery}, \textbf{Dataproc}, \textbf{GKE}) and utilized \textbf{Argo} workflows to automate services in the pipeline}
%     %   \item {Contributed to }
%         % \item {Designed \& implemented API layer of automatic video preview \& thumbnail generator microservice using \textbf{Django}, \textbf{Docker}, and \textbf{Kubernetes}}
%         % \item {Productionized, maintained, and monitored automatic video preview \& thumbnail generator service on \textbf{GCP} (\textbf{Stackdriver}, \textbf{GKE}, \textbf{Pub/Sub}, \textbf{GCS}, \textbf{Cloud SQL}), \textbf{Codefresh}, and \textbf{Grafana} to save company money and support our brands (e.g. \textbf{CBS News}, \textbf{CBS All Access}) to use service}
%       \end{cvitems}
%     }




% %---------------------------------------------------------
% \cventry
%     % {Applied Machine Learning Group, Machine Learning Engineer} % Job title
%     {Data Science Intern, Applied Machine Learning Group} % second title
%     {CBS Interactive} % Organization
%     {San Francisco, CA} % Location
%     {June 2019 - Dec. 2019} % second date
%     {
%       \begin{cvitems} % Description(s) of tasks/responsibilities
%  		\item {Researched various text summarization methods (TextRank, Pointer Generator) and created production-ready text summarizer application consisting of Chrome extension (\textbf{JavaScript/HTML/CSS}) with Python backend server deployed on \textbf{Google App Engine} using \textbf{Docker}}
%   		\item {Built ML microservice for automatic person recognition from videos using \textbf{GCP} (\textbf{AutoML, Pub/Sub, CloudSQL, GCS}), \textbf{Docker}, and \textbf{Django}}
% %   		\item {Researched, implemented, and benchmarked different text summarization methods (TextRank, Pointer Generator)}
% % 		\item {Created a Google Chrome extension (JavaScript/HTML/CSS) with a Python backend server to demonstrate the different text summarization methods}
%       \end{cvitems}
%     }

%---------------------------------------------------------
% \cventry
%     % {Audio Test \& Data Team, Data Engineer} % Job title
%     {Data Engineer, Audio Test \& Data Team} % Job title
%     {Knowles Intelligent Audio} % Organization
%     {Mountain View, CA} % Location
%     {Oct. 2016 - May 2019} % Date(s)
%     {
%       \begin{cvitems} % Description(s) of tasks/responsibilities
% %         \item {Developed Matlab and Python tools to automate data collection}
% %         \item {Developed Python tools \& scripts to validate, process, clean, and upload large amounts of critical customer and internal audio data to MongoDB database to enable other cross-functional engineering teams to use data for testing and model training}
% % 	\item {Designed data pipeline using Python for verification, processing, and transfer of large amounts of critical audio data into \textbf{MongoDB} database}
%  		\item {Designed \& implemented data pipeline using \textbf{Python} for verification, processing, and transfer of large amounts of critical audio data into \textbf{MongoDB} database to enable cross-functional engineering teams and created \textbf{REST API} for CRUD operations on the database}
%  	       \item {Developed programmatic tools for audio data acquisition, data analysis and data processing using \textbf{Python} and \textbf{MATLAB}}
%         % \item {Developed tools to automate data acquisition \& data processing and created clear documentation \& detailed procedures for usage of these tools}
%         % \item {Contributed to \textbf{Python REST API} for CRUD operations and designed functional test suites for \textbf{MongoDB} database}
%         % \item {Proficiently used}
%         % \item {Designed and implemented automated functional and usability test suites for web-based database application using Selenium and Python (unittest, pytest)}
%       \end{cvitems}
%     }



% %---------------------------------------------------------
\cventryupdate
    % {Audio Test \& Data Team, Data Engineer} % Job title
    {Data Engineer} % Job title
    {Knowles Intelligent Audio} % Organization
    {Mountain View, CA} % Location
    {Feb. 2017 - May 2019} % Date(s)
    {Test Engineering Intern} % second title
    {Oct. 2016 - Feb. 2017} % second date
    {
      \begin{cvitems} % Description(s) of tasks/responsibilities
%         \item {Developed Matlab and Python tools to automate data collection}
%         \item {Developed Python tools \& scripts to validate, process, clean, and upload large amounts of critical customer and internal audio data to MongoDB database to enable other cross-functional engineering teams to use data for testing and model training}
% 	\item {Designed data pipeline using Python for verification, processing, and transfer of large amounts of critical audio data into \textbf{MongoDB} database}
 		\item {Designed \& implemented \textbf{Python} data pipeline for verification, processing, and transfer of large amounts of critical audio data into \textbf{MongoDB} database to enable cross-functional engineering teams and created \textbf{REST API} for CRUD operations on the database}
 	      % \item {Developed programmatic tools for audio data acquisition, data analysis and data processing using \textbf{Python} \& \textbf{MATLAB}}
        % \item {Developed tools to automate data acquisition \& data processing and created clear documentation \& detailed procedures for usage of these tools}
        % \item {Contributed to \textbf{Python REST API} for CRUD operations and designed functional test suites for \textbf{MongoDB} database}
        % \item {Proficiently used}
        % \item {Designed and implemented automated functional and usability test suites for web-based database application using Selenium and Python (unittest, pytest)}
      \end{cvitems}
    }

%---------------------------------------------------------
\end{cventries}
